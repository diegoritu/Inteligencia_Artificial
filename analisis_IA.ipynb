{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <ins> Análisis de datos de accidentes viales </ins>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1- Importando el dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Se importa el dataset con Pandas y se definen los nombres de las columnas, para poder referenciarlas posteriormente._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, roc_curve\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "#Importamos solamente los atributos que nos interesa analizar.\n",
    "data_frame = pd.read_csv(\"Accidente.csv\", header=0 ,usecols=[\"Severity\",\"Junction\", \"Stop\", \"Traffic_Signal\",\"Traffic_Calming\", \"Roundabout\", \"No_Exit\", \"Give_Way\", \"Bump\", \"Amenity\", \"Side\", \"Temperature(F)\", \"Sunrise_Sunset\",\"Visibility(mi)\", \"Pressure(in)\",\"Humidity(%)\",\"Wind_Speed(mph)\", \"Distance(mi)\", \"Crossing\", \"Railway\", \"Start_Time\", \"Weather_Condition\", \"Wind_Direction\", \"Precipitation(in)\"])\n",
    "#data_frame = pd.read_csv(\"Accidente.csv\", header=0)\n",
    "#Definimos la semilla que se utilizará a lo largo del proyecto.\n",
    "random_state=15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2- Data Processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _2.1. Se eliminan todos los duplicados, seaplica One-Hot-Encoding para tratar los datos categóricos de forma adecuada para los modelos y se convierten todos los booleanos a 0 y 1._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Severity</th>\n",
       "      <th>Start_Time</th>\n",
       "      <th>Distance(mi)</th>\n",
       "      <th>Temperature(F)</th>\n",
       "      <th>Humidity(%)</th>\n",
       "      <th>Pressure(in)</th>\n",
       "      <th>Visibility(mi)</th>\n",
       "      <th>Wind_Direction</th>\n",
       "      <th>Wind_Speed(mph)</th>\n",
       "      <th>Precipitation(in)</th>\n",
       "      <th>Weather_Condition</th>\n",
       "      <th>Amenity</th>\n",
       "      <th>Bump</th>\n",
       "      <th>Crossing</th>\n",
       "      <th>Give_Way</th>\n",
       "      <th>Junction</th>\n",
       "      <th>No_Exit</th>\n",
       "      <th>Railway</th>\n",
       "      <th>Roundabout</th>\n",
       "      <th>Stop</th>\n",
       "      <th>Traffic_Calming</th>\n",
       "      <th>Traffic_Signal</th>\n",
       "      <th>Sunrise_Sunset_Day</th>\n",
       "      <th>Sunrise_Sunset_Night</th>\n",
       "      <th>Side_</th>\n",
       "      <th>Side_L</th>\n",
       "      <th>Side_R</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>2016-02-08 05:46:00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>36.9</td>\n",
       "      <td>91.0</td>\n",
       "      <td>29.68</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Calm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.02</td>\n",
       "      <td>Light Rain</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2016-02-08 06:07:59</td>\n",
       "      <td>0.01</td>\n",
       "      <td>37.9</td>\n",
       "      <td>100.0</td>\n",
       "      <td>29.65</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Calm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Light Rain</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2016-02-08 06:49:27</td>\n",
       "      <td>0.01</td>\n",
       "      <td>36.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>29.67</td>\n",
       "      <td>10.0</td>\n",
       "      <td>SW</td>\n",
       "      <td>3.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Overcast</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2016-02-08 07:23:34</td>\n",
       "      <td>0.01</td>\n",
       "      <td>35.1</td>\n",
       "      <td>96.0</td>\n",
       "      <td>29.64</td>\n",
       "      <td>9.0</td>\n",
       "      <td>SW</td>\n",
       "      <td>4.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mostly Cloudy</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2016-02-08 07:39:07</td>\n",
       "      <td>0.01</td>\n",
       "      <td>36.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>29.65</td>\n",
       "      <td>6.0</td>\n",
       "      <td>SW</td>\n",
       "      <td>3.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mostly Cloudy</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Severity           Start_Time  Distance(mi)  Temperature(F)  Humidity(%)  \\\n",
       "0         3  2016-02-08 05:46:00          0.01            36.9         91.0   \n",
       "1         2  2016-02-08 06:07:59          0.01            37.9        100.0   \n",
       "2         2  2016-02-08 06:49:27          0.01            36.0        100.0   \n",
       "3         3  2016-02-08 07:23:34          0.01            35.1         96.0   \n",
       "4         2  2016-02-08 07:39:07          0.01            36.0         89.0   \n",
       "\n",
       "   Pressure(in)  Visibility(mi) Wind_Direction  Wind_Speed(mph)  \\\n",
       "0         29.68            10.0           Calm              NaN   \n",
       "1         29.65            10.0           Calm              NaN   \n",
       "2         29.67            10.0             SW              3.5   \n",
       "3         29.64             9.0             SW              4.6   \n",
       "4         29.65             6.0             SW              3.5   \n",
       "\n",
       "   Precipitation(in) Weather_Condition  Amenity  Bump  Crossing  Give_Way  \\\n",
       "0               0.02        Light Rain        0     0         0         0   \n",
       "1               0.00        Light Rain        0     0         0         0   \n",
       "2                NaN          Overcast        0     0         0         0   \n",
       "3                NaN     Mostly Cloudy        0     0         0         0   \n",
       "4                NaN     Mostly Cloudy        0     0         0         0   \n",
       "\n",
       "   Junction  No_Exit  Railway  Roundabout  Stop  Traffic_Calming  \\\n",
       "0         0        0        0           0     0                0   \n",
       "1         0        0        0           0     0                0   \n",
       "2         0        0        0           0     0                0   \n",
       "3         0        0        0           0     0                0   \n",
       "4         0        0        0           0     0                0   \n",
       "\n",
       "   Traffic_Signal  Sunrise_Sunset_Day  Sunrise_Sunset_Night  Side_   Side_L  \\\n",
       "0               0                   0                     1       0       0   \n",
       "1               0                   0                     1       0       1   \n",
       "2               1                   0                     1       0       0   \n",
       "3               0                   0                     1       0       0   \n",
       "4               1                   1                     0       0       0   \n",
       "\n",
       "   Side_R  \n",
       "0       1  \n",
       "1       0  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame.drop_duplicates(inplace=True)\n",
    "\n",
    "#Reemplazamos todos los True y False por 1 y 0, respectivamente. Esto aplica a las columnas Junction, Stop, Crossing Railway y Traffic_Signal\n",
    "data_frame = data_frame.replace([True, False], [1, 0])\n",
    "\n",
    "#Columna \"Sunrise_Sunset\"\n",
    "data_frame_dummies_sunrise_sunset = pd.get_dummies(data_frame['Sunrise_Sunset'])\n",
    "data_frame_dummies_sunrise_sunset = data_frame_dummies_sunrise_sunset.rename(columns=lambda s: \"Sunrise_Sunset_\" + s)\n",
    "data_frame = data_frame.drop(['Sunrise_Sunset'], axis = 1)\n",
    "data_frame = data_frame.join(data_frame_dummies_sunrise_sunset)\n",
    "\n",
    "#Columna \"Side\"\n",
    "data_frame_dummies_side = pd.get_dummies(data_frame['Side'])\n",
    "data_frame_dummies_side = data_frame_dummies_side.rename(columns=lambda s: \"Side_\" + s)\n",
    "data_frame = data_frame.drop(['Side'], axis = 1)\n",
    "data_frame = data_frame.join(data_frame_dummies_side)\n",
    "\n",
    "data_frame.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _2.2. Acotamos los valores de condiciones climáticas y aplicamos one-hot-encoding sobre \"Weather\\_Condition\"._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Lluvia' 'Nublado' 'Nieve' 'Niebla' 'Clear' nan 'Humo' 'Tormenta'\n",
      " 'Despejado' 'Tormenta de arena' 'Ventoso' 'Granizada' 'Tornado']\n"
     ]
    }
   ],
   "source": [
    "data_frame.loc[data_frame[\"Weather_Condition\"].str.contains(\"Thunder|T-Storm\", na=False), \"Weather_Condition\"] = \"Tormenta\"\n",
    "data_frame.loc[data_frame[\"Weather_Condition\"].str.contains(\"Snow|Sleet|Wintry\", na=False), \"Weather_Condition\"] = \"Nieve\"\n",
    "data_frame.loc[data_frame[\"Weather_Condition\"].str.contains(\"Rain|Drizzle|Shower\", na=False), \"Weather_Condition\"] = \"Lluvia\"\n",
    "data_frame.loc[data_frame[\"Weather_Condition\"].str.contains(\"Wind|Squalls\", na=False), \"Weather_Condition\"] = \"Ventoso\"\n",
    "data_frame.loc[data_frame[\"Weather_Condition\"].str.contains(\"Hail|Pellets\", na=False), \"Weather_Condition\"] = \"Granizada\"\n",
    "data_frame.loc[data_frame[\"Weather_Condition\"].str.contains(\"Fair\", na=False), \"Weather_Condition\"] = \"Despejado\"\n",
    "data_frame.loc[data_frame[\"Weather_Condition\"].str.contains(\"Cloud|Overcast\", na=False), \"Weather_Condition\"] = \"Nublado\"\n",
    "data_frame.loc[data_frame[\"Weather_Condition\"].str.contains(\"Mist|Haze|Fog\", na=False), \"Weather_Condition\"] = \"Niebla\"\n",
    "data_frame.loc[data_frame[\"Weather_Condition\"].str.contains(\"Sand|Dust\", na=False), \"Weather_Condition\"] = \"Tormenta de arena\"\n",
    "data_frame.loc[data_frame[\"Weather_Condition\"].str.contains(\"Smoke|Volcanic Ash\", na=False), \"Weather_Condition\"] = \"Humo\"\n",
    "data_frame.loc[data_frame[\"Weather_Condition\"].str.contains(\"N/A Precipitation\", na=False), \"Weather_Condition\"] = np.nan\n",
    "\n",
    "print(data_frame[\"Weather_Condition\"].unique())\n",
    "\n",
    "#Columna \"Weather_Condition\"\n",
    "data_frame_dummies_weather_condition = pd.get_dummies(data_frame['Weather_Condition'])\n",
    "data_frame_dummies_weather_condition = data_frame_dummies_weather_condition.rename(columns=lambda s: \"Weather_Condition\" + s)\n",
    "data_frame = data_frame.drop(['Weather_Condition'], axis = 1)\n",
    "data_frame = data_frame.join(data_frame_dummies_weather_condition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame.loc[data_frame[\"Wind_Direction\"] == \"CALM\", \"Wind_Direction\"] = \"Calmo\"\n",
    "data_frame.loc[data_frame[\"Wind_Direction\"] == \"VAR\", \"Wind_Direction\"] = \"Variable\"\n",
    "data_frame.loc[data_frame[\"Wind_Direction\"] == \"East\", \"Wind_Direction\"] = \"E\"\n",
    "data_frame.loc[data_frame[\"Wind_Direction\"] == \"North\", \"Wind_Direction\"] = \"N\"\n",
    "data_frame.loc[data_frame[\"Wind_Direction\"] == \"South\", \"Wind_Direction\"] = \"S\"\n",
    "data_frame.loc[data_frame[\"Wind_Direction\"] == \"West\", \"Wind_Direction\"] = \"O\"\n",
    "\n",
    "data_frame[\"Wind_Direction\"] = data_frame[\"Wind_Direction\"].map(lambda x : x if len(x) != 3 else x[1:], na_action=\"ignore\")\n",
    "\n",
    "#Columna \"Wind_Direction\"\n",
    "data_frame_dummies_wind_direction = pd.get_dummies(data_frame['Wind_Direction'])\n",
    "data_frame_dummies_wind_direction = data_frame_dummies_wind_direction.rename(columns=lambda s: \"Wind_Direction\" + s)\n",
    "data_frame = data_frame.drop(['Wind_Direction'], axis = 1)\n",
    "data_frame = data_frame.join(data_frame_dummies_wind_direction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _2.3. Subdividimos la fecha del accidente y eliminamos la característica original._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame[\"Start_Time\"] = pd.to_datetime(data_frame[\"Start_Time\"])\n",
    "\n",
    "# Extract year, month, weekday and day\n",
    "data_frame[\"Year\"] = data_frame[\"Start_Time\"].dt.year\n",
    "data_frame[\"Month\"] = data_frame[\"Start_Time\"].dt.month\n",
    "data_frame[\"Weekday\"] = data_frame[\"Start_Time\"].dt.weekday\n",
    "data_frame[\"Day\"] = data_frame[\"Start_Time\"].dt.day\n",
    "\n",
    "# Extract hour and minute\n",
    "data_frame[\"Hour\"] = data_frame[\"Start_Time\"].dt.hour\n",
    "data_frame[\"Minute\"] = data_frame[\"Start_Time\"].dt.minute\n",
    "\n",
    "data_frame.drop(\"Start_Time\",axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _2.4. Se verifican los valores de Presión y Visibilidad._\n",
    "#### _Como estos poseen valores incoherentes, se eliminan todas las instancias en que valen 0._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pressure(in)</th>\n",
       "      <th>Visibility(mi)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2896774.00</td>\n",
       "      <td>2879393.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>29.83</td>\n",
       "      <td>9.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.71</td>\n",
       "      <td>2.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.02</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>29.82</td>\n",
       "      <td>10.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>29.98</td>\n",
       "      <td>10.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>30.11</td>\n",
       "      <td>10.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>33.04</td>\n",
       "      <td>140.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Pressure(in)  Visibility(mi)\n",
       "count    2896774.00      2879393.00\n",
       "mean          29.83            9.15\n",
       "std            0.71            2.89\n",
       "min            0.02            0.06\n",
       "25%           29.82           10.00\n",
       "50%           29.98           10.00\n",
       "75%           30.11           10.00\n",
       "max           33.04          140.00"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Verificamos los valores de Presión y Visibilidad\n",
    "data_frame[[\"Pressure(in)\", \"Visibility(mi)\"]].describe().round(2)\n",
    "\n",
    "\"\"\"Tras ver el resultado, y considerando que es imposible una visibilidad de 0 millas y\n",
    "   una presión de 0 pulgadas, procedemos a eliminar estos posibles valores del dataset.\"\"\"\n",
    "\n",
    "data_frame = data_frame[data_frame[\"Pressure(in)\"] != 0]\n",
    "data_frame = data_frame[data_frame[\"Visibility(mi)\"] != 0]\n",
    "data_frame[[\"Pressure(in)\", \"Visibility(mi)\"]].describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _2.5. Se completan todos los datos faltantes de Visibilidad, Presión, Humedad, Temperatura y Velocidad del Viento con la media de cada característica._\n",
    "#### _Esto se hace dado que de otra forma deberían eliminarse las filas, y representaría perder una gran cantidad de datos, que como se verá posteriormente, por tener tan pocas instancias de accidentes con Severidad 1, afectaría de forma negativa a la aplicación de los modelos._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rellenamos valores nulos con la media:\n",
    "\n",
    "params = [\"Visibility(mi)\", \"Pressure(in)\", \"Humidity(%)\", \"Temperature(F)\", \"Wind_Speed(mph)\", \"Precipitation(in)\"]\n",
    "\n",
    "data_frame[params] = data_frame[params].fillna(data_frame[params].mean())\n",
    "\n",
    "data_frame.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _2.6. Se realiza una matriz de correlación para ver claramente qué características son más útiles para predecir la severidad de un accidente, y cuáles no tienen una fuerte relación con esta característica._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'correlation_matrix = data_frame.corr()\\n\\nplt.figure(figsize=(15, 10))\\nsns.heatmap(correlation_matrix, vmin=-1, vmax=1, cmap=\"seismic\")\\nplt.gca().patch.set(hatch=\"X\", edgecolor=\"#666\")\\nplt.show()'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"correlation_matrix = data_frame.corr()\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.heatmap(correlation_matrix, vmin=-1, vmax=1, cmap=\"seismic\")\n",
    "plt.gca().patch.set(hatch=\"X\", edgecolor=\"#666\")\n",
    "plt.show()\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _2.7. Eliminamos las características que tienen muy baja correlación con la severidad._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame.drop(\"Crossing\",axis=1, inplace=True)\n",
    "data_frame.drop(\"Side_L\",axis=1, inplace=True)\n",
    "data_frame.drop(\"Traffic_Signal\",axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _2.8. Se reemplaza el dataset por una muestra en que se almacena la misma cantidad de datos de cada tipo de severidad. El tamaño de esta muestra está dado por la cantidad de ocurrencias de Severity = 1, que es la clase que aparece la menor cantidad de veces._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Toma la cantidad de datos de severidad 1 (la clase con menos instancias de todas)\n",
    "size = len(data_frame[data_frame[\"Severity\"]==1].index)\n",
    "df = pd.DataFrame()\n",
    "for i in range(1,5):\n",
    "    #Selecciona el tipo de severidad\n",
    "    sev = data_frame[data_frame[\"Severity\"]==i]\n",
    "    #Toma una muestra de tamaño \"size\" de la severidad seleccionada\n",
    "    df = df.append(sev.sample(size, random_state=random_state))\n",
    "\n",
    "data_frame = df\n",
    "\n",
    "data_frame.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _2.9. Se escalan las variables continuas, para disminuir lo más posible el impacto que puedan tener los outliers sobre los modelos aplicados posteriormente._\n",
    "#### _Se aplica el algoritmo MinMaxScaler, elegido por ser el scaler que mejor performance dio posteriormente al entrenar los modelos. En una primera instancia, se probaron los scalers MinMax, Robust y Standard._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "minMaxScaler = StandardScaler()\n",
    "data_min_max_scaler = minMaxScaler.fit_transform(data_frame[[\"Humidity(%)\", \"Pressure(in)\", \"Wind_Speed(mph)\", \"Distance(mi)\", \"Visibility(mi)\", \"Temperature(F)\", \"Year\", \"Month\", \"Weekday\", \"Day\", \"Hour\", \"Minute\", \"Precipitation(in)\"]])\n",
    "data_min_max_scaler = pd.DataFrame(data_min_max_scaler, columns =[\"Humidity(%)\", \"Pressure(in)\", \"Wind_Speed(mph)\", \"Distance(mi)\", \"Visibility(mi)\", \"Temperature(F)\", \"Year\", \"Month\", \"Weekday\", \"Day\", \"Hour\", \"Minute\", \"Precipitation(in)\"])\n",
    "\n",
    "data_frame = data_frame.drop([\"Humidity(%)\", \"Pressure(in)\", \"Wind_Speed(mph)\", \"Distance(mi)\", \"Visibility(mi)\", \"Temperature(F)\", \"Year\", \"Month\", \"Weekday\", \"Day\", \"Hour\", \"Minute\", \"Precipitation(in)\"], axis = 1)\n",
    "data_frame = data_frame.join(data_min_max_scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3- Aplicación de Modelos - Linear SVC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _3.1. Se toman todos los datos que quedaron en el dataset luego de haber tomado una muestra con la misma cantidad de ocurrencias de cada tipo de severidad en los pasos anteriores._\n",
    "#### _En total se cuenta con 3868 datos. Como al entrenar al modelo demoraba mucho, se procedió a limitar la cantidad de datos a 3000, tomando una segunda muestra, a partir de la primera_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sample = data_frame.sample(3_000, random_state=random_state)\\ny_sample = sample[\"Severity\"]\\nx_sample = sample.drop(\"Severity\", axis=1)\\n\\nprint(y_sample.value_counts())\\nprint(\"-\"*50)\\nprint(x_sample.count())'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"sample = data_frame.sample(3_000, random_state=random_state)\n",
    "y_sample = sample[\"Severity\"]\n",
    "x_sample = sample.drop(\"Severity\", axis=1)\n",
    "\n",
    "print(y_sample.value_counts())\n",
    "print(\"-\"*50)\n",
    "print(x_sample.count())\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _3.2. Se aplica Grid Search para conseguir el parámetro de C que mejor ajusta al modelo._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'parameters = {\\'C\\': [0.1, 0.2, 0.5],\\'kernel\\': [\\'linear\\']}\\n#parameters = [{\"kernel\": [\"linear\", \"rbf\", \"sigmoid\"], \"C\": [.2, .5, .8, 1.]}, {\"kernel\": [\"poly\"], \"C\": [.2, .5, .8, 1.], \"degree\": [2, 3, 4]}]\\nmodelSVC = svm.SVC(random_state=random_state)\\n\\ngrid = GridSearchCV(modelSVC, parameters, n_jobs=-1)\\n\\ngrid.fit(x_sample, y_sample)\\n\\nprint(\"Mejor valor de C:\")\\nprint(grid.best_params_)\\nprint(\"Train score:\", grid.score(x_sample, y_sample))'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"parameters = {'C': [0.1, 0.2, 0.5],'kernel': ['linear']}\n",
    "#parameters = [{\"kernel\": [\"linear\", \"rbf\", \"sigmoid\"], \"C\": [.2, .5, .8, 1.]}, {\"kernel\": [\"poly\"], \"C\": [.2, .5, .8, 1.], \"degree\": [2, 3, 4]}]\n",
    "modelSVC = svm.SVC(random_state=random_state)\n",
    "\n",
    "grid = GridSearchCV(modelSVC, parameters, n_jobs=-1)\n",
    "\n",
    "grid.fit(x_sample, y_sample)\n",
    "\n",
    "print(\"Mejor valor de C:\")\n",
    "print(grid.best_params_)\n",
    "print(\"Train score:\", grid.score(x_sample, y_sample))\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _3.3. Una vez conseguido el mejor valor de C, se lo utiliza para entrenar al modelo con el mismo._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xtrain, xtest, ytrain, ytest = train_test_split(x_sample, y_sample, stratify=y_sample, test_size=0.2, random_state=random_state)\\n\\n#linear_svc = svm.SVC(**grid.best_params_,random_state=random_state)\\nlinear_svc = svm.SVC(C=0.2,gamma= 0.1,kernel=\"linear\",random_state=random_state)\\n\\nlinear_svc.fit(xtrain, ytrain)\\n\\nprint(\"Train score:\", linear_svc.score(xtrain, ytrain))\\nprint(\"Test score:\", linear_svc.score(xtest, ytest))\\nprint(\"YTRAIN MAX: {}\".format(np.max(ytrain)))\\nprint(\"YTRAIN MIN: {}\".format(np.min(ytrain)))\\nprint(\"YTEST MAX: {}\".format(np.max(ytest)))\\nprint(\"YTEST MIN: {}\".format(np.min(ytest)))\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"xtrain, xtest, ytrain, ytest = train_test_split(x_sample, y_sample, stratify=y_sample, test_size=0.2, random_state=random_state)\n",
    "\n",
    "#linear_svc = svm.SVC(**grid.best_params_,random_state=random_state)\n",
    "linear_svc = svm.SVC(C=0.2,gamma= 0.1,kernel=\"linear\",random_state=random_state)\n",
    "\n",
    "linear_svc.fit(xtrain, ytrain)\n",
    "\n",
    "print(\"Train score:\", linear_svc.score(xtrain, ytrain))\n",
    "print(\"Test score:\", linear_svc.score(xtest, ytest))\n",
    "print(\"YTRAIN MAX: {}\".format(np.max(ytrain)))\n",
    "print(\"YTRAIN MIN: {}\".format(np.min(ytrain)))\n",
    "print(\"YTEST MAX: {}\".format(np.max(ytest)))\n",
    "print(\"YTEST MIN: {}\".format(np.min(ytest)))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _3.4. Se predicen los resultados de Severity de acuerdo a los datos de testing, y se proyecta su resultado en una matriz de confusión._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'y_pred = linear_svc.predict(xtest)\\nconfmat = confusion_matrix(y_true=ytest, y_pred=y_pred)\\n\\nprint(\"YPRED MAX: {}\".format(np.max(y_pred)))\\nprint(\"YPRED MIN: {}\".format(np.min(y_pred)))\\nprint(\"YTRAIN MAX: {}\".format(np.max(ytrain)))\\nprint(\"YTRAIN MIN: {}\".format(np.min(ytrain)))\\nprint(\"YTEST MAX: {}\".format(np.max(ytest)))\\nprint(\"YTEST MIN: {}\".format(np.min(ytest)))\\n\\nindex = [\"Actual Severity 1\", \"Actual Severity 2\", \"Actual Severity 3\", \"Actual Severity 4\"]\\ncolumns = [\"Predicted Severity 1\", \"Predicted Severity 2\", \"Predicted Severity 3\", \"Predicted Severity 4\"]\\nconf_matrix = pd.DataFrame(data=confmat, columns=columns, index=index)\\nplt.figure(figsize=(8, 5))\\nsns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"YlGnBu\")\\nplt.title(\"Confusion Matrix - Support Vector Machine\")\\nplt.show()\\nprint(ytest.value_counts())'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"y_pred = linear_svc.predict(xtest)\n",
    "confmat = confusion_matrix(y_true=ytest, y_pred=y_pred)\n",
    "\n",
    "print(\"YPRED MAX: {}\".format(np.max(y_pred)))\n",
    "print(\"YPRED MIN: {}\".format(np.min(y_pred)))\n",
    "print(\"YTRAIN MAX: {}\".format(np.max(ytrain)))\n",
    "print(\"YTRAIN MIN: {}\".format(np.min(ytrain)))\n",
    "print(\"YTEST MAX: {}\".format(np.max(ytest)))\n",
    "print(\"YTEST MIN: {}\".format(np.min(ytest)))\n",
    "\n",
    "index = [\"Actual Severity 1\", \"Actual Severity 2\", \"Actual Severity 3\", \"Actual Severity 4\"]\n",
    "columns = [\"Predicted Severity 1\", \"Predicted Severity 2\", \"Predicted Severity 3\", \"Predicted Severity 4\"]\n",
    "conf_matrix = pd.DataFrame(data=confmat, columns=columns, index=index)\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"YlGnBu\")\n",
    "plt.title(\"Confusion Matrix - Support Vector Machine\")\n",
    "plt.show()\n",
    "print(ytest.value_counts())\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4- Aplicación de Modelos - Árboles de Decisión."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _4.1. Se toman todos los datos que quedaron en el dataset luego de haber tomado una muestra con la misma cantidad de ocurrencias de cada tipo de severidad en los pasos anteriores._\n",
    "#### _En total se cuenta con 3868 datos._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'y_sample = data_frame[\"Severity\"]\\nx_sample = data_frame.drop(\"Severity\", axis=1)\\n\\nprint(y_sample.value_counts())\\nprint(\"-\"*50)\\nprint(x_sample.count())'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"y_sample = data_frame[\"Severity\"]\n",
    "x_sample = data_frame.drop(\"Severity\", axis=1)\n",
    "\n",
    "print(y_sample.value_counts())\n",
    "print(\"-\"*50)\n",
    "print(x_sample.count())\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _4.2. Se aplica Grid Search para conseguir la máxima profundidad que mejor ajusta al árbol._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xtrain, xtest, ytrain, ytest = train_test_split(x_sample, y_sample, stratify=y_sample, test_size=0.2, random_state=random_state)\\n\\ndtc = DecisionTreeClassifier(random_state=random_state)\\nparameters = [{\"criterion\": [\"entropy\"], \"max_depth\": [5, 10, 15, 30]}]\\ngrid = GridSearchCV(dtc, parameters, verbose=5, n_jobs=-1)\\ngrid.fit(xtrain, ytrain)\\n\\nprint(\"Best parameters scores:\")\\nprint(grid.best_params_)\\nprint(\"Train score:\", grid.score(xtrain, ytrain))\\nprint(\"Validation score:\", grid.score(xtest, ytest))'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"xtrain, xtest, ytrain, ytest = train_test_split(x_sample, y_sample, stratify=y_sample, test_size=0.2, random_state=random_state)\n",
    "\n",
    "dtc = DecisionTreeClassifier(random_state=random_state)\n",
    "parameters = [{\"criterion\": [\"entropy\"], \"max_depth\": [5, 10, 15, 30]}]\n",
    "grid = GridSearchCV(dtc, parameters, verbose=5, n_jobs=-1)\n",
    "grid.fit(xtrain, ytrain)\n",
    "\n",
    "print(\"Best parameters scores:\")\n",
    "print(grid.best_params_)\n",
    "print(\"Train score:\", grid.score(xtrain, ytrain))\n",
    "print(\"Validation score:\", grid.score(xtest, ytest))\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _4.3. Se predicen los resultados de Severity de acuerdo a los datos de testing, y se proyecta su resultado en una matriz de confusión._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'y_pred = dtc.predict(xtest)\\nconfmat = confusion_matrix(y_true=ytest, y_pred=y_pred)\\n\\nindex = [\"Actual Severity 1\", \"Actual Severity 2\", \"Actual Severity 3\", \"Actual Severity 4\"]\\ncolumns = [\"Predicted Severity 1\", \"Predicted Severity 2\", \"Predicted Severity 3\", \"Predicted Severity 4\"]\\nconf_matrix = pd.DataFrame(data=confmat, columns=columns, index=index)\\nplt.figure(figsize=(8, 5))\\nsns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"YlGnBu\")\\nplt.title(\"Confusion Matrix - Decision Tree\")\\nplt.show()'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"y_pred = dtc.predict(xtest)\n",
    "confmat = confusion_matrix(y_true=ytest, y_pred=y_pred)\n",
    "\n",
    "index = [\"Actual Severity 1\", \"Actual Severity 2\", \"Actual Severity 3\", \"Actual Severity 4\"]\n",
    "columns = [\"Predicted Severity 1\", \"Predicted Severity 2\", \"Predicted Severity 3\", \"Predicted Severity 4\"]\n",
    "conf_matrix = pd.DataFrame(data=confmat, columns=columns, index=index)\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"YlGnBu\")\n",
    "plt.title(\"Confusion Matrix - Decision Tree\")\n",
    "plt.show()\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _4.4. Se representa gráficamente el árbol de decisión._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfig, ax = plt.subplots(figsize=(20, 10))\\nplot_tree(dtc, max_depth=4, fontsize=10, feature_names=X_train.columns.to_list(), class_names = True, filled=True)\\nplt.show()\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "plot_tree(dtc, max_depth=4, fontsize=10, feature_names=X_train.columns.to_list(), class_names = True, filled=True)\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5- Aplicación de Modelos - Redes Neuronales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _5.1. Se toman todos los datos que quedaron en el dataset luego de haber tomado una muestra con la misma cantidad de ocurrencias de cada tipo de severidad en los pasos anteriores._\n",
    "#### _En total se cuenta con 3868 datos._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    967\n",
      "2    967\n",
      "3    967\n",
      "4    967\n",
      "Name: Severity, dtype: int64\n",
      "--------------------------------------------------\n",
      "index                                 3868\n",
      "Amenity                               3868\n",
      "Bump                                  3868\n",
      "Give_Way                              3868\n",
      "Junction                              3868\n",
      "No_Exit                               3868\n",
      "Railway                               3868\n",
      "Roundabout                            3868\n",
      "Stop                                  3868\n",
      "Traffic_Calming                       3868\n",
      "Sunrise_Sunset_Day                    3868\n",
      "Sunrise_Sunset_Night                  3868\n",
      "Side_                                 3868\n",
      "Side_R                                3868\n",
      "Weather_ConditionClear                3868\n",
      "Weather_ConditionDespejado            3868\n",
      "Weather_ConditionGranizada            3868\n",
      "Weather_ConditionHumo                 3868\n",
      "Weather_ConditionLluvia               3868\n",
      "Weather_ConditionNiebla               3868\n",
      "Weather_ConditionNieve                3868\n",
      "Weather_ConditionNublado              3868\n",
      "Weather_ConditionTormenta             3868\n",
      "Weather_ConditionTormenta de arena    3868\n",
      "Weather_ConditionTornado              3868\n",
      "Weather_ConditionVentoso              3868\n",
      "Wind_DirectionCalm                    3868\n",
      "Wind_DirectionCalmo                   3868\n",
      "Wind_DirectionE                       3868\n",
      "Wind_DirectionN                       3868\n",
      "Wind_DirectionNE                      3868\n",
      "Wind_DirectionNW                      3868\n",
      "Wind_DirectionO                       3868\n",
      "Wind_DirectionS                       3868\n",
      "Wind_DirectionSE                      3868\n",
      "Wind_DirectionSW                      3868\n",
      "Wind_DirectionVariable                3868\n",
      "Wind_DirectionW                       3868\n",
      "Humidity(%)                           3868\n",
      "Pressure(in)                          3868\n",
      "Wind_Speed(mph)                       3868\n",
      "Distance(mi)                          3868\n",
      "Visibility(mi)                        3868\n",
      "Temperature(F)                        3868\n",
      "Year                                  3868\n",
      "Month                                 3868\n",
      "Weekday                               3868\n",
      "Day                                   3868\n",
      "Hour                                  3868\n",
      "Minute                                3868\n",
      "Precipitation(in)                     3868\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "y_sample = data_frame[\"Severity\"]\n",
    "x_sample = data_frame.drop(\"Severity\", axis=1)\n",
    "\n",
    "print(y_sample.value_counts())\n",
    "print(\"-\"*50)\n",
    "print(x_sample.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _5.2. Se aplica Grid Search para definir la cantidad de iteraciones (o learning rate) óptimos, la cantidad de capas ocultas, la función resolvente y la de activación._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mlp = MLPClassifier(random_state=random_state)\\nparameters = [{\"hidden_layer_sizes\": [(64, 32), (32, 64, 32)], \"max_iter\": [200], \"solver\": [\"sgd\", \"adam\"], \"activation\": [\"tanh\", \"relu\"]}]\\ngrid = GridSearchCV(mlp, parameters, verbose=5, n_jobs=-1)\\n\\ngrid.fit(x_sample, y_sample)\\n\\nprint(\"Best parameters scores:\")\\nprint(grid.best_params_)\\nprint(\"Train score:\", grid.score(xtrain, ytrain))\\nprint(\"Validation score:\", grid.score(xtest, ytest))'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain, xtest, ytrain, ytest = train_test_split(x_sample, y_sample, stratify=y_sample, test_size=0.2, random_state=random_state)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Dense(500,input_dim = xtrain.shape[1], activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(400, activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(300, activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "\"\"\"mlp = MLPClassifier(random_state=random_state)\n",
    "parameters = [{\"hidden_layer_sizes\": [(64, 32), (32, 64, 32)], \"max_iter\": [200], \"solver\": [\"sgd\", \"adam\"], \"activation\": [\"tanh\", \"relu\"]}]\n",
    "grid = GridSearchCV(mlp, parameters, verbose=5, n_jobs=-1)\n",
    "\n",
    "grid.fit(x_sample, y_sample)\n",
    "\n",
    "print(\"Best parameters scores:\")\n",
    "print(grid.best_params_)\n",
    "print(\"Train score:\", grid.score(xtrain, ytrain))\n",
    "print(\"Validation score:\", grid.score(xtest, ytest))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=\"sgd\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "97/97 [==============================] - 1s 4ms/step - loss: 114454560094830985216.0000 - accuracy: 0.2586\n",
      "Epoch 2/30\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 2.1290 - accuracy: 0.2498\n",
      "Epoch 3/30\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 2.0283 - accuracy: 0.2498\n",
      "Epoch 4/30\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 1.9460 - accuracy: 0.2498\n",
      "Epoch 5/30\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 1.8784 - accuracy: 0.2498\n",
      "Epoch 6/30\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 1.8226 - accuracy: 0.2498\n",
      "Epoch 7/30\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 1.7762 - accuracy: 0.2498\n",
      "Epoch 8/30\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 1.7372 - accuracy: 0.2498\n",
      "Epoch 9/30\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 1.7042 - accuracy: 0.2498\n",
      "Epoch 10/30\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 1.6761 - accuracy: 0.2498\n",
      "Epoch 11/30\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 1.6519 - accuracy: 0.2463\n",
      "Epoch 12/30\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 1.6308 - accuracy: 0.2498\n",
      "Epoch 13/30\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 1.6124 - accuracy: 0.2473\n",
      "Epoch 14/30\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 1.5962 - accuracy: 0.2498\n",
      "Epoch 15/30\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 1.5817 - accuracy: 0.2372\n",
      "Epoch 16/30\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 1.5689 - accuracy: 0.2498\n",
      "Epoch 17/30\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 1.5573 - accuracy: 0.2498\n",
      "Epoch 18/30\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 1.5468 - accuracy: 0.2408\n",
      "Epoch 19/30\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 1.5373 - accuracy: 0.2427\n",
      "Epoch 20/30\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 1.5285 - accuracy: 0.2498\n",
      "Epoch 21/30\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 1.5205 - accuracy: 0.2424\n",
      "Epoch 22/30\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 1.5131 - accuracy: 0.2498\n",
      "Epoch 23/30\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 1.5063 - accuracy: 0.2492\n",
      "Epoch 24/30\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 1.5000 - accuracy: 0.2453\n",
      "Epoch 25/30\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 1.4940 - accuracy: 0.2498\n",
      "Epoch 26/30\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 1.4885 - accuracy: 0.2498\n",
      "Epoch 27/30\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 1.4833 - accuracy: 0.2498\n",
      "Epoch 28/30\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 1.4784 - accuracy: 0.2498\n",
      "Epoch 29/30\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 1.4739 - accuracy: 0.2456\n",
      "Epoch 30/30\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 1.4695 - accuracy: 0.2476\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(xtrain, ytrain, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 3ms/step - loss: 1.4673 - accuracy: 0.2506\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.467265248298645, 0.2506459951400757]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(xtest, ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _5.3. Se predicen los resultados de Severity de acuerdo a los datos de testing, y se proyecta su resultado en una matriz de confusión._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'y_pred = model.evaluate(xtest, ytest)\\nconfmat = confusion_matrix(y_true=ytest, y_pred=y_pred)\\n\\nindex = [\"Actual Severity 1\", \"Actual Severity 2\", \"Actual Severity 3\", \"Actual Severity 4\"]\\ncolumns = [\"Predicted Severity 1\", \"Predicted Severity 2\", \"Predicted Severity 3\", \"Predicted Severity 4\"]\\nconf_matrix = pd.DataFrame(data=confmat, columns=columns, index=index)\\nplt.figure(figsize=(8, 5))\\nsns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"YlGnBu\")\\nplt.title(\"Confusion Matrix - Multi Layer Perceptron\")\\nplt.show()'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"y_pred = model.evaluate(xtest, ytest)\n",
    "confmat = confusion_matrix(y_true=ytest, y_pred=y_pred)\n",
    "\n",
    "index = [\"Actual Severity 1\", \"Actual Severity 2\", \"Actual Severity 3\", \"Actual Severity 4\"]\n",
    "columns = [\"Predicted Severity 1\", \"Predicted Severity 2\", \"Predicted Severity 3\", \"Predicted Severity 4\"]\n",
    "conf_matrix = pd.DataFrame(data=confmat, columns=columns, index=index)\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"YlGnBu\")\n",
    "plt.title(\"Confusion Matrix - Multi Layer Perceptron\")\n",
    "plt.show()\"\"\""
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "02de5609ef162b6bcc51dd429fd31b9a1f163c414526741362218fd7f2e354ad"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
